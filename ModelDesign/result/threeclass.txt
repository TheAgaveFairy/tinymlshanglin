device is -------------- cuda
IEGMNetSimple5a(
  (conv1): Sequential(
    (0): Conv2d(1, 2, kernel_size=(6, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv2): Sequential(
    (0): Conv2d(2, 3, kernel_size=(5, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv3): Sequential(
    (0): Conv2d(3, 5, kernel_size=(4, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv4): Sequential(
    (0): Conv2d(5, 10, kernel_size=(4, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv5): Sequential(
    (0): Conv2d(10, 10, kernel_size=(4, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear): Linear(in_features=370, out_features=3, bias=True)
)
conv1.0.weight torch.Size([2, 1, 6, 1])
conv1.0.bias torch.Size([2])
conv1.2.weight torch.Size([2])
conv1.2.bias torch.Size([2])
conv2.0.weight torch.Size([3, 2, 5, 1])
conv2.0.bias torch.Size([3])
conv2.2.weight torch.Size([3])
conv2.2.bias torch.Size([3])
conv3.0.weight torch.Size([5, 3, 4, 1])
conv3.0.bias torch.Size([5])
conv3.2.weight torch.Size([5])
conv3.2.bias torch.Size([5])
conv4.0.weight torch.Size([10, 5, 4, 1])
conv4.0.bias torch.Size([10])
conv4.2.weight torch.Size([10])
conv4.2.bias torch.Size([10])
conv5.0.weight torch.Size([10, 10, 4, 1])
conv5.0.bias torch.Size([10])
conv5.2.weight torch.Size([10])
conv5.2.bias torch.Size([10])
linear.weight torch.Size([3, 370])
linear.bias torch.Size([3])
Training Dataset loading finish.
Start training
Epoch:  0
0.5720270050431104
Test || Loss:0.01010 Acc: 0.35644
        Precision: nan Recall: 1.00000 F1: nan FB: nan 
Epoch:  1
0.6673173905970392
Test || Loss:0.00219 Acc: 0.80267
        Precision: 2.04695 Recall: 1.97408 F1: 2.00986 FB: 1.98824 
Epoch:  2
0.6967219782007483
Test || Loss:0.00848 Acc: 0.35698
        Precision: nan Recall: 1.16217 F1: nan FB: nan 
Epoch:  3
0.7058321132259638
Test || Loss:0.00305 Acc: 0.80764
        Precision: 2.16043 Recall: 1.97677 F1: 2.06452 FB: 2.01096 
Epoch:  4
0.7248251179437124
Test || Loss:0.00176 Acc: 0.83893
        Precision: 2.57423 Recall: 2.08622 F1: 2.30467 FB: 2.16844 
Epoch:  5
0.7296648771758582
Test || Loss:0.00283 Acc: 0.80907
        Precision: 2.26005 Recall: 2.01780 F1: 2.13207 FB: 2.06201 
Epoch:  6
0.7337725719863348
Test || Loss:0.00265 Acc: 0.81991
        Precision: 2.17729 Recall: 2.00858 F1: 2.08954 FB: 2.04020 
Epoch:  7
0.7227102651700016
Test || Loss:0.00162 Acc: 0.84178
        Precision: 2.36504 Recall: 2.25412 F1: 2.30825 FB: 2.27547 
Epoch:  8
0.7428013665202537
Test || Loss:0.00183 Acc: 0.83484
        Precision: 2.44941 Recall: 2.10902 F1: 2.26651 FB: 2.16931 
Epoch:  9
0.751301447860745
Test || Loss:0.00170 Acc: 0.83253
        Precision: 2.38525 Recall: 2.13908 F1: 2.25547 FB: 2.18416 
Epoch:  10
0.744997559785261
Test || Loss:0.00189 Acc: 0.82773
        Precision: 2.32983 Recall: 2.14447 F1: 2.23331 FB: 2.17914 
Epoch:  11
0.7480884984545306
Test || Loss:0.00164 Acc: 0.83271
        Precision: 2.29945 Recall: 2.17520 F1: 2.23560 FB: 2.19896 
Epoch:  12
0.7521961932650073
Test || Loss:0.00280 Acc: 0.69476
        Precision: 1.99406 Recall: 1.91632 F1: 1.95442 FB: 1.93138 
Epoch:  13
0.7558158451277046
Test || Loss:0.00197 Acc: 0.80836
        Precision: 2.32057 Recall: 2.09338 F1: 2.20113 FB: 2.13519 
Epoch:  14
0.7571172929884497
Test || Loss:0.00165 Acc: 0.83680
        Precision: 2.35196 Recall: 2.21303 F1: 2.28038 FB: 2.23948 
Epoch:  15
0.7558971856190011
Test || Loss:0.00246 Acc: 0.81778
        Precision: 2.29236 Recall: 2.12442 F1: 2.20520 FB: 2.15601 
Epoch:  16
0.7575239954449325
Test || Loss:0.00191 Acc: 0.78524
        Precision: 2.11445 Recall: 2.11545 F1: 2.11495 FB: 2.11525 
Epoch:  17
0.7615910200097609
Test || Loss:0.00205 Acc: 0.82649
        Precision: 2.31107 Recall: 2.13999 F1: 2.22224 FB: 2.17215 
Epoch:  18
0.7612249877989263
Test || Loss:0.00168 Acc: 0.83911
        Precision: 2.38152 Recall: 2.19413 F1: 2.28399 FB: 2.22921 
Epoch:  19
0.7627704571335611
Test || Loss:0.00493 Acc: 0.40036
        Precision: 1.58382 Recall: 1.51942 F1: 1.55095 FB: 1.53188 
Epoch:  20
0.747275093541565
Test || Loss:0.00172 Acc: 0.84196
        Precision: 2.39409 Recall: 2.18742 F1: 2.28610 FB: 2.22585 
Epoch:  21
0.765617374328941
Test || Loss:0.00187 Acc: 0.83627
        Precision: 2.39334 Recall: 2.17609 F1: 2.27955 FB: 2.21633 
Epoch:  22
0.7699284203676591
Test || Loss:0.00199 Acc: 0.81244
        Precision: 2.29708 Recall: 2.10111 F1: 2.19473 FB: 2.13758 
Epoch:  23
0.7621604034488368
Test || Loss:0.00183 Acc: 0.82044
        Precision: 2.30797 Recall: 2.17228 F1: 2.23807 FB: 2.19813 
Epoch:  24
0.7662274280136652
Test || Loss:0.00202 Acc: 0.80853
        Precision: 2.27530 Recall: 2.13071 F1: 2.20064 FB: 2.15814 
Epoch:  25
0.7676102163657068
Test || Loss:0.00210 Acc: 0.82311
        Precision: 2.29154 Recall: 2.16405 F1: 2.22597 FB: 2.18840 
Epoch:  26
0.7680575890678379
Test || Loss:0.00197 Acc: 0.81671
        Precision: 2.26710 Recall: 2.11881 F1: 2.19045 FB: 2.14689 
Epoch:  27
0.7714332194566456
Test || Loss:0.00200 Acc: 0.83004
        Precision: 2.34819 Recall: 2.15354 F1: 2.24665 FB: 2.18984 
Epoch:  28
0.7674475353831137
Test || Loss:0.00241 Acc: 0.82631
        Precision: 2.39627 Recall: 2.09732 F1: 2.23685 FB: 2.15099 
Epoch:  29
0.7704978038067349
Test || Loss:0.00202 Acc: 0.82702
        Precision: 2.35097 Recall: 2.14773 F1: 2.24476 FB: 2.18552 
Epoch:  30
0.7683016105417276
Test || Loss:0.00183 Acc: 0.83307
        Precision: 2.39810 Recall: 2.17484 F1: 2.28102 FB: 2.21610 
Epoch:  31
0.7723279648609077
Test || Loss:0.00202 Acc: 0.81564
        Precision: 2.30138 Recall: 2.09127 F1: 2.19130 FB: 2.13017 
Epoch:  32
0.7770457133561087
Test || Loss:0.00242 Acc: 0.79360
        Precision: 2.25135 Recall: 2.07378 F1: 2.15892 FB: 2.10702 
Epoch:  33
0.7598015292012363
Test || Loss:0.00186 Acc: 0.81742
        Precision: 2.27551 Recall: 2.15319 F1: 2.21266 FB: 2.17659 
Epoch:  34
0.773507401984708
Test || Loss:0.00190 Acc: 0.82844
        Precision: 2.33331 Recall: 2.15073 F1: 2.23831 FB: 2.18493 
Epoch:  35
0.7672441841548723
Test || Loss:0.00189 Acc: 0.82204
        Precision: 2.30233 Recall: 2.12221 F1: 2.20860 FB: 2.15594 
Epoch:  36
0.7701317715959004
Test || Loss:0.00195 Acc: 0.82169
        Precision: 2.28319 Recall: 2.11364 F1: 2.19515 FB: 2.14551 
Epoch:  37
0.7739547746868392
Test || Loss:0.00185 Acc: 0.81671
        Precision: 2.28921 Recall: 2.23777 F1: 2.26320 FB: 2.24787 
Epoch:  38
0.7624857654140231
Test || Loss:0.00248 Acc: 0.79858
        Precision: 2.18917 Recall: 2.07258 F1: 2.12928 FB: 2.09490 
Epoch:  39
0.7717179111761835
Test || Loss:0.00250 Acc: 0.82204
        Precision: 2.31774 Recall: 2.11318 F1: 2.21074 FB: 2.15116 
Epoch:  40
0.7670408329266308
Test || Loss:0.00194 Acc: 0.82329
        Precision: 2.29433 Recall: 2.15078 F1: 2.22024 FB: 2.17803 
Epoch:  41
0.7696843988937693
Test || Loss:0.00205 Acc: 0.83253
        Precision: 2.36190 Recall: 2.16653 F1: 2.26000 FB: 2.20297 
Epoch:  42
0.7748088498454531
Test || Loss:0.00246 Acc: 0.81813
        Precision: 2.27252 Recall: 2.12910 F1: 2.19848 FB: 2.15632 
Epoch:  43
0.7704164633154384
Test || Loss:0.00200 Acc: 0.79573
        Precision: 2.21706 Recall: 2.08536 F1: 2.14919 FB: 2.11043 
Epoch:  44
0.7653733528550513
Test || Loss:0.00224 Acc: 0.81636
        Precision: 2.24723 Recall: 2.11160 F1: 2.17730 FB: 2.13740 
Epoch:  45
0.7749308605823979
Test || Loss:0.00202 Acc: 0.80000
        Precision: 2.15308 Recall: 2.11446 F1: 2.13359 FB: 2.12207 
Epoch:  46
0.7773710753212949
Test || Loss:0.00203 Acc: 0.82169
        Precision: 2.25627 Recall: 2.15406 F1: 2.20398 FB: 2.17376 
Epoch:  47
0.772938018545632
Test || Loss:0.00192 Acc: 0.80587
        Precision: 2.20098 Recall: 2.10834 F1: 2.15367 FB: 2.12624 
Epoch:  48
0.7702537823328452
Test || Loss:0.00177 Acc: 0.82382
        Precision: 2.29417 Recall: 2.19720 F1: 2.24463 FB: 2.21593 
Epoch:  49
0.7757035952497153
Test || Loss:0.00209 Acc: 0.82809
        Precision: 2.35466 Recall: 2.17320 F1: 2.26030 FB: 2.20722 
Best Fb: 2.275466563164078
Finish training
TRAINING COMPLETE. TESTING NOW.
device is -------------- cuda:0
[[2570    1   69]
 [  56 1780  169]
 [ 382  213  385]]
0.8417777777777777
              precision    recall  f1-score   support

         AFb       0.85      0.97      0.91      2640
         AFt       0.89      0.89      0.89      2005
          SR       0.62      0.39      0.48       980
         SVT       0.00      0.00      0.00         0
         VFb       0.00      0.00      0.00         0
         VFt       0.00      0.00      0.00         0
         VPD       0.00      0.00      0.00         0
          VT       0.00      0.00      0.00         0

   micro avg       0.84      0.84      0.84      5625
   macro avg       0.30      0.28      0.29      5625
weighted avg       0.83      0.84      0.83      5625

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 2, 623, 1]              14
              ReLU-2            [-1, 2, 623, 1]               0
       BatchNorm2d-3            [-1, 2, 623, 1]               4
            Conv2d-4            [-1, 3, 310, 1]              33
              ReLU-5            [-1, 3, 310, 1]               0
       BatchNorm2d-6            [-1, 3, 310, 1]               6
            Conv2d-7            [-1, 5, 154, 1]              65
              ReLU-8            [-1, 5, 154, 1]               0
       BatchNorm2d-9            [-1, 5, 154, 1]              10
           Conv2d-10            [-1, 10, 76, 1]             210
             ReLU-11            [-1, 10, 76, 1]               0
      BatchNorm2d-12            [-1, 10, 76, 1]              20
           Conv2d-13            [-1, 10, 37, 1]             410
             ReLU-14            [-1, 10, 37, 1]               0
      BatchNorm2d-15            [-1, 10, 37, 1]              20
           Linear-16                    [-1, 3]           1,113
================================================================
Total params: 1,905
Trainable params: 1,905
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.09
Params size (MB): 0.01
Estimated Total Size (MB): 0.11
----------------------------------------------------------------
