Minor Six Classes: AFb,AFt,SVT,VFb,VFt,VPD. NO SR or VT!
device is -------------- cuda
IEGMNetSimple5aMultiMinor(
  (conv1): Sequential(
    (0): Conv2d(1, 2, kernel_size=(6, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv2): Sequential(
    (0): Conv2d(2, 3, kernel_size=(5, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv3): Sequential(
    (0): Conv2d(3, 5, kernel_size=(4, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv4): Sequential(
    (0): Conv2d(5, 10, kernel_size=(4, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv5): Sequential(
    (0): Conv2d(10, 10, kernel_size=(4, 1), stride=(2, 1))
    (1): ReLU(inplace=True)
    (2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear): Linear(in_features=370, out_features=6, bias=True)
)
conv1.0.weight torch.Size([2, 1, 6, 1])
conv1.0.bias torch.Size([2])
conv1.2.weight torch.Size([2])
conv1.2.bias torch.Size([2])
conv2.0.weight torch.Size([3, 2, 5, 1])
conv2.0.bias torch.Size([3])
conv2.2.weight torch.Size([3])
conv2.2.bias torch.Size([3])
conv3.0.weight torch.Size([5, 3, 4, 1])
conv3.0.bias torch.Size([5])
conv3.2.weight torch.Size([5])
conv3.2.bias torch.Size([5])
conv4.0.weight torch.Size([10, 5, 4, 1])
conv4.0.bias torch.Size([10])
conv4.2.weight torch.Size([10])
conv4.2.bias torch.Size([10])
conv5.0.weight torch.Size([10, 10, 4, 1])
conv5.0.bias torch.Size([10])
conv5.2.weight torch.Size([10])
conv5.2.bias torch.Size([10])
linear.weight torch.Size([6, 370])
linear.bias torch.Size([6])
Training Dataset loading finish.
Start training
Epoch:  0
0.33963241004400724
Test || Loss:0.00698 Acc: 0.13163
        Precision: nan Recall: 1.00000 F1: nan FB: nan 
Epoch:  1
0.6202433341962206
Test || Loss:0.00808 Acc: 0.60918
        Precision: nan Recall: 2.24998 F1: nan FB: nan 
Epoch:  2
0.7056691690396065
Test || Loss:0.00853 Acc: 0.51020
        Precision: 2.36858 Recall: nan F1: nan FB: nan 
Epoch:  3
0.7540771421175252
Test || Loss:0.00704 Acc: 0.69898
        Precision: 3.06020 Recall: nan F1: nan FB: nan 
Epoch:  4
0.7776339632410044
Test || Loss:0.00702 Acc: 0.65714
        Precision: 2.77945 Recall: nan F1: nan FB: nan 
Epoch:  5
0.7929070670463371
Test || Loss:0.00790 Acc: 0.64490
        Precision: 2.53033 Recall: nan F1: nan FB: nan 
Epoch:  6
0.8115454310121667
Test || Loss:0.00783 Acc: 0.65306
        Precision: 2.66666 Recall: nan F1: nan FB: nan 
Epoch:  7
0.8314781258089567
Test || Loss:0.00931 Acc: 0.53673
        Precision: 2.31164 Recall: nan F1: nan FB: nan 
Epoch:  8
0.8265596686513073
Test || Loss:0.01138 Acc: 0.52959
        Precision: 2.64495 Recall: nan F1: nan FB: nan 
Epoch:  9
0.8413150401242557
Test || Loss:0.01252 Acc: 0.39388
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  10
0.8558115454310121
Test || Loss:0.01147 Acc: 0.40918
        Precision: 2.04351 Recall: nan F1: nan FB: nan 
Epoch:  11
0.8785917680559151
Test || Loss:0.01158 Acc: 0.41633
        Precision: 2.40660 Recall: nan F1: nan FB: nan 
Epoch:  12
0.88169816205022
Test || Loss:0.01138 Acc: 0.62041
        Precision: 2.64482 Recall: nan F1: nan FB: nan 
Epoch:  13
0.8539994822676676
Test || Loss:0.02157 Acc: 0.33163
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  14
0.8798860988868755
Test || Loss:0.01343 Acc: 0.49286
        Precision: 2.19336 Recall: nan F1: nan FB: nan 
Epoch:  15
0.9055138493398913
Test || Loss:0.01324 Acc: 0.45204
        Precision: 2.09329 Recall: nan F1: nan FB: nan 
Epoch:  16
0.8917939425317111
Test || Loss:0.01522 Acc: 0.49388
        Precision: 2.23330 Recall: nan F1: nan FB: nan 
Epoch:  17
0.8894641470359824
Test || Loss:0.01362 Acc: 0.51735
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  18
0.9021485891793942
Test || Loss:0.01393 Acc: 0.53469
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  19
0.9047372508413151
Test || Loss:0.01439 Acc: 0.51939
        Precision: 2.13035 Recall: nan F1: nan FB: nan 
Epoch:  20
0.9086202433341962
Test || Loss:0.01918 Acc: 0.34898
        Precision: 1.72402 Recall: nan F1: nan FB: nan 
Epoch:  21
0.8866166192078695
Test || Loss:0.01390 Acc: 0.58163
        Precision: 2.33510 Recall: nan F1: nan FB: nan 
Epoch:  22
0.9137975666580378
Test || Loss:0.01684 Acc: 0.37041
        Precision: 1.32629 Recall: nan F1: nan FB: nan 
Epoch:  23
0.9192337561480715
Test || Loss:0.01404 Acc: 0.54592
        Precision: 2.22290 Recall: nan F1: nan FB: nan 
Epoch:  24
0.9171628268185348
Test || Loss:0.01554 Acc: 0.45408
        Precision: 2.13605 Recall: nan F1: nan FB: nan 
Epoch:  25
0.911208904996117
Test || Loss:0.01702 Acc: 0.38980
        Precision: 1.51290 Recall: nan F1: nan FB: nan 
Epoch:  26
0.9125032358270774
Test || Loss:0.01556 Acc: 0.49184
        Precision: 1.92762 Recall: nan F1: nan FB: nan 
Epoch:  27
0.9137975666580378
Test || Loss:0.01102 Acc: 0.57857
        Precision: 2.47203 Recall: nan F1: nan FB: nan 
Epoch:  28
0.8969712658555526
Test || Loss:0.01580 Acc: 0.41327
        Precision: 2.39892 Recall: nan F1: nan FB: nan 
Epoch:  29
0.8972301320217447
Test || Loss:0.01709 Acc: 0.51327
        Precision: 1.94988 Recall: nan F1: nan FB: nan 
Epoch:  30
0.9311415997929071
Test || Loss:0.01713 Acc: 0.46122
        Precision: 2.14055 Recall: nan F1: nan FB: nan 
Epoch:  31
0.9075847786694279
Test || Loss:0.02182 Acc: 0.34898
        Precision: 1.28897 Recall: nan F1: nan FB: nan 
Epoch:  32
0.9174216929847269
Test || Loss:0.01553 Acc: 0.45510
        Precision: 2.15809 Recall: nan F1: nan FB: nan 
Epoch:  33
0.9225990163085684
Test || Loss:0.01679 Acc: 0.50816
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  34
0.9368366554491327
Test || Loss:0.01505 Acc: 0.55102
        Precision: 1.98703 Recall: nan F1: nan FB: nan 
Epoch:  35
0.9339891276210199
Test || Loss:0.01552 Acc: 0.57245
        Precision: 2.19283 Recall: nan F1: nan FB: nan 
Epoch:  36
0.9223401501423764
Test || Loss:0.01880 Acc: 0.46020
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  37
0.9285529381309863
Test || Loss:0.01723 Acc: 0.46531
        Precision: 2.16527 Recall: nan F1: nan FB: nan 
Epoch:  38
0.9169039606523427
Test || Loss:0.02285 Acc: 0.32653
        Precision: 1.38661 Recall: nan F1: nan FB: nan 
Epoch:  39
0.9156096298213824
Test || Loss:0.01419 Acc: 0.55612
        Precision: 2.17379 Recall: nan F1: nan FB: nan 
Epoch:  40
0.9314004659590992
Test || Loss:0.01365 Acc: 0.52041
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  41
0.9197514884804556
Test || Loss:0.01581 Acc: 0.55612
        Precision: 2.41225 Recall: nan F1: nan FB: nan 
Epoch:  42
0.9132798343256536
Test || Loss:0.01795 Acc: 0.48265
        Precision: 2.01072 Recall: nan F1: nan FB: nan 
Epoch:  43
0.9233756148071447
Test || Loss:0.02454 Acc: 0.33367
        Precision: 1.48286 Recall: nan F1: nan FB: nan 
Epoch:  44
0.9301061351281388
Test || Loss:0.02436 Acc: 0.36633
        Precision: 1.76980 Recall: nan F1: nan FB: nan 
Epoch:  45
0.9314004659590992
Test || Loss:0.01591 Acc: 0.60102
        Precision: 2.25611 Recall: nan F1: nan FB: nan 
Epoch:  46
0.9358011907843645
Test || Loss:0.01918 Acc: 0.48265
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  47
0.898006730520321
Test || Loss:0.01889 Acc: 0.43367
        Precision: nan Recall: nan F1: nan FB: nan 
Epoch:  48
0.9311415997929071
Test || Loss:0.01820 Acc: 0.41837
        Precision: 1.70203 Recall: nan F1: nan FB: nan 
Epoch:  49
0.9225990163085684
Test || Loss:0.01667 Acc: 0.49490
        Precision: 1.97632 Recall: nan F1: nan FB: nan 
Best Fb: 0.0
Finish training
TRAINING COMPLETE. TESTING NOW.
device is -------------- cuda:0
[[166  72  26  45   0   0]
 [  0   0   0   0   0   0]
 [  0   0 129   0   0   0]
 [ 83   0   0 189  11   6]
 [  0   0   0  95   0   0]
 [140  17   0   0   0   1]]
0.49489795918367346
              precision    recall  f1-score   support

         AFb       0.43      0.54      0.48       309
         AFt       0.00      0.00      0.00         0
         SVT       0.83      1.00      0.91       129
         VFb       0.57      0.65      0.61       289
         VFt       0.00      0.00      0.00        95
         VPD       0.14      0.01      0.01       158

   micro avg       0.49      0.49      0.49       980
   macro avg       0.25      0.27      0.25       980
weighted avg       0.44      0.49      0.45       980

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 2, 623, 1]              14
              ReLU-2            [-1, 2, 623, 1]               0
       BatchNorm2d-3            [-1, 2, 623, 1]               4
            Conv2d-4            [-1, 3, 310, 1]              33
              ReLU-5            [-1, 3, 310, 1]               0
       BatchNorm2d-6            [-1, 3, 310, 1]               6
            Conv2d-7            [-1, 5, 154, 1]              65
              ReLU-8            [-1, 5, 154, 1]               0
       BatchNorm2d-9            [-1, 5, 154, 1]              10
           Conv2d-10            [-1, 10, 76, 1]             210
             ReLU-11            [-1, 10, 76, 1]               0
      BatchNorm2d-12            [-1, 10, 76, 1]              20
           Conv2d-13            [-1, 10, 37, 1]             410
             ReLU-14            [-1, 10, 37, 1]               0
      BatchNorm2d-15            [-1, 10, 37, 1]              20
           Linear-16                    [-1, 6]           2,226
================================================================
Total params: 3,018
Trainable params: 3,018
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.09
Params size (MB): 0.01
Estimated Total Size (MB): 0.11
----------------------------------------------------------------
